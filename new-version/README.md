## Introduction

The code in this repository is intended for use in the [Inscriptions of Israel / Palestine project](http://library.brown.edu/cds/projects/iip/search/) (IIP).
It uses Python and LXML to generate word lists from [epidoc](http://www.stoa.org/epidoc/gl/latest/) files and includes a simple web interface.

The purpose of this part of the project is to create a list of lemmata found in the IIP texts.
In plain language, that means that it makes a dictionary or glossary of all of the words in all of the texts in the IIP project by grouping together multiple instances of words into a single entry.
This collection of lemmata will henceforth be referred to as the "index".
The languages which appear in the texts are: Latin, Greek, Hebrew, and Aramaic.
Ideally, there should be an index for each language.
These indices provide a reference to the vocabulary of the texts and a tool for research purposes.
In the final version, the index should operate as (or like) a webpage with a list of lemmata, each of which links to all of its attestations in the original texts.
This user-facing version of the index should be created using the data generated by this program, therefore the program must output all information needed to generate that final product.

To illustrate how this might work, consider the following English example:

     <XML>She was in the study, but now she is studying in the living room.</XML>

A simplified version of the index might look like:

| Lemma Number | Lemma  | Part of Speech | Translation | Count |
|--------------|--------|:--------------:|-------------|:-----:|
| 1            | be     |        V       | "be"        |   2   |
| 2            | but    |      CONJ      | "but"       |   1   |
| 3            | in     |      PREP      | "in"        |   2   |
| 4            | living |       ADJ      | "living"    |   1   |
| 5            | now    |       ADV      | "now"       |   1   |
| 6            | room   |        N       | "room"      |   1   |
| 7            | she    |       PN       | "she"       |   2   |
| 8            | study  |        V       | "study"     |   1   |
| 9            | study  |        N       | "study"     |   1   |
| 10           | the    |       ART      | "the"       |   2   |

The XML formatting of the original text is processed and the words are organized into a table.
The "Translation" column is redundant in this case, but would be needed if the source text were written in any language other than English.
The "Part of Speech" column uses a standard set of abbreviations to represent the part of speech of each lemma in its original context.
Though there are 14 words in the sample text, there are 10 in the index, because some words are repeated in the text.
These multiple instances are grouped into a single entry in the index.
Parts of speech are labeled, even when they would be ambiguous outside of the original context (e.g. "study" might be a verb, but in this case it is clearly a noun, while "studying" is clearly an inflected form of the verb).
This process is known as "Part of Speech Tagging" or "POS Tagging".
POS Tagging requires as input the words in their context, since context is needed to deal with ambiguity (e.g. "study" is clearly not a verb in this case, because "in the study" wouldn't make sense with a verb).

Different inflected forms are grouped together in a single lemma (e.g. "was" and "is" become "be").
This process is known as "Lemmatization".
Lemmatization benefits from having parts of speech tagged in advance, because it is often the case that a single word may have multiple possible lemmata.
This is especially true in heavily inflected languages, such as Latin and Greek.
To cite an English example, the word "drunk" should be lemmatized as "drink" when used as the past participle of the verb, as in: "I have drunk three glasses of water today",
while the word "drunk" should be lemmatized as "drunk" when used as an adjective, as in: "The drunk man sat at the bar".
To process these two cases, POS Tagger would return "drink, V" in the first and "drunk, ADJ" in the second.
The lemmatizer returns both possibilities at all times, and the program must choose which possibility is correct in each case based on the information from the POS Tagger.
The final process of organizing all of the information about each word (location in text, part of speech, lemma) is a fairly simple task, once all of the necessary information has been collected.
It amounts to grouping all word instances with identical features into a single lemma and alphabetizing the list.

## Workflow

The current structure of the project is designed to facilitate an iterative process of building a complete system and improving the final output by separating the larger task into smaller, discrete steps.
For instance, XML Parsing, POS Tagging, and Lemmatization naturally form separate steps, and are therefore separated in the current version of the program.
This avoids the problems caused by tangling the various parts of the project together, and makes the code easier to improve and maintain.
As long as each step uses the output produced by the previous step as its input and produces the necessary output for the subsequent step, the internal operation of each step is independent of all others.
The steps may be thought of as modules or atomic units, which can be modified and replaced at will.

The current program separates these atomic units into individual sub-programs (called "steps"),
each of which saves its results into a set of data files, one data file per text.
The directories in which the files are saved mirror the stepwise process.
For instance, the input data are simply the raw IIP texts in XML format, up-to-date versions of which may be found in the [iip-texts repo](https://github.com/Brown-University-Library/iip-texts) in the directory called "epidoc-files".
The directory containing these data for the current program is called "Step 0 Output - 1 Input".
That is, it is the output of some hypothetical, prior step, which may be thought of as "Step 0", and it is the input for the first step of this program.
The rest of the directories continue in this fashion.
The final result will be a single index file in the last directory named in this fashion, i.e. "Step n Output - ∞ Input" or similar.
(At present, the final consolidation step is step 4).
The purpose of this is to ensure that the entire process can be maintained efficiently
  by making it easy to modify distinct components,
  and to make changes on isolated steps without repeatedly processing the entire dataset from scratch.

The broadly-defined steps are as follows:

1. **XML Parser** – Extract words from text (in sequence) from XML inscriptions
2. **Part-of-Speech Tagger** – Tag parts of speech
3. **Lemmatizer** – Lemmatize results of previous step.
4. **Word List Compiler** – Generate word list on lemmatized word table

### Updating (and Debugging)

Running the entire program, in all its steps, on all data files, is very slow.
The Lemmatizer is particularly slow (currently step 3).
For this reason, it may be advantageous to run the program on a subset of the texts,
  whether for coding/debugging purposes, or because a single text has been modified and only those few changes need to be processed.
In order to do this, run each script step with command-line arguments containing the files to be processed.
For example, in order to run step 1 on only one text, use the command:

    python step1.py --texts "ashk0003a.xml"

The same applies to all subsequent steps, except the final step, which will not accept command line arguments as it must be run collectively on all files.
A script which runs this program in response to individual changes in the texts might use the command line argument method to run a quick update.

Texts may be modified and updated individually by other authors at any time.
In such cases, the program should be run only on those files that have been changed, rather than rerunning the program from scratch on the entire dataset.
The changed files will be added to the output-input directories of each step.
This is why the output-input directories contain a data file for each text
 – changing one text requires that the intermediate data files be recreated, but that can be done individually.
The new, updated data files for each text are added or changed in each output-input directory, but the other files are left alone.
Only in the final consolidation step is it necessary to run the full script on all of the data.
However, this step is relatively quick, since it is only consolidating data that has already been processed.



## Setup

1. Create a directory for this project into which you will place all of the files.
2. Clone the repository
3. The current version is in a subdirectory named "new version test" (for now, you can change it)
4. The program requires several additional components, which should also be added to the main project folder.
  1. [TreeTagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)
  2. [PerseusDL/morpheus](https://github.com/PerseusDL/morpheus)
  3. [Morpheus-Perseids](https://github.com/perseids-tools/morpheus-perseids)
  4. [Docker](https://www.docker.com) (for Morpheus-Perseids)

Setup for step 2:
1. if not already included in the repository or you are working with a different operating system, download the TreeTagger, as well as a latin library.
2. create bin, cmd, and lib directories in the same folder,and check the tree-tagger-latin file for what should be in each folder
3. in line 23 of step2.py, when specifying the TAGDIR variable to the treetagger, set it to the directory where the bin,cmd, and lib directories are located

Setup for step3:
1. download PerseusDL/morpheus, Morpheus-Perseids, from the links above
2. for both, follow the links in the readme to compile
3. when compiling PerseusDL/morpheus, follow instructions here https://github.com/PerseusDL/morpheus/issues/16 if running make produces an error

Possible errors for morpheus:
1. if running make produces an Fclose error, go into the each of the makefiles of the subdirectories (anal, auto, gener, gkdict, etc) and replace the line "CC = gcc" with "CC=gcc -Wno-return-type"

## Running the scripts

To run the scripts, just calling "python [filename]" should be sufficient - running "python [filename] --help" also gives a list of commands.
To change/specify where each step should take its input and output, change strPathIn and strPathOut variable at the beginning of each file.
As of now, the process is incomplete.
The workflow described here may need to be modified before it is complete, so no script yet exists to run the entire project at once.
When each step is complete, it is a simple matter to create a shell script that runs all steps in sequence, or runs each step with a file to update and then runs the final step, etc.
For now, while the program is a work in progress, run each script as a discrete step, checking the output each time to ensure that the program is working correctly.

## Viewing results

Initial results are stored in the file: "Step 4 Output.csv"
